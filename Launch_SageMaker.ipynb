{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd54ffbe",
   "metadata": {},
   "source": [
    "## SageMaker Yolo\n",
    "\n",
    "This notebook details how to launch YOLO training on SageMaker, including how to train with multinode, EFA, and SageMaker Debugger.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, make sure to follow the SageMaker instructions in the repo README in order to build your Docker image for training. You should upload this image to your ECR repo.\n",
    "\n",
    "Second, you need to have the COCO data and labels stored on S3. The dataset is available at the [COCO website](https://cocodataset.org/#home). You'll also need to convert the COCO labels into the YOLO format. The COCO labels are stored in a json file with boxes in the format (x1, y1, x2, y2) describing the corners of the box. YOLO expects a text files for each image with a line for each object specifying category, and box in the format (x, y, w, h) where x and y are the box center. For example, if image `1234.jpg` has 2 people and 1 bicycle, the file `1234.txt` will have three lines\n",
    "\n",
    "```\n",
    "1 0.500000 0.842305 1.000000 0.315391\n",
    "1 0.531875 0.465930 0.712083 0.782422\n",
    "2 0.540583 0.837477 0.457042 0.300234\n",
    "```\n",
    "\n",
    "If you want to generate these labels yourself, or have your own dataset, [this repo](https://github.com/qwirky-yuzu/COCO-to-YOLO) contains a conversion script. If you just want to train on YOLO, you can find labels [here](). Your data should have the following file structure:\n",
    "\n",
    "```\n",
    "coco    \n",
    "│\n",
    "└───annotations\n",
    "│   │   instances_val2017.json\n",
    "│   \n",
    "└───images\n",
    "│   │\n",
    "│   └───train2017\n",
    "│   │   │   0001.jpg\n",
    "│   │   │   0002.jpg\n",
    "│   │   │   ...\n",
    "│   │\n",
    "│   └───val2017\n",
    "│       │   5001.jpg\n",
    "│       │   5002.jpg\n",
    "│       │   ...\n",
    "│\n",
    "└───labels\n",
    "│   │\n",
    "│   └───train2017\n",
    "│   │   │   0001.txt\n",
    "│   │   │   0002.txt\n",
    "│   │   │   ...\n",
    "│   │\n",
    "│   └───val2017\n",
    "│       │   5001.txt\n",
    "│       │   5002.txt\n",
    "│       │   ...\n",
    "│   train2017.txt (list of all image files)\n",
    "│   val2017.txt \n",
    "```\n",
    "\n",
    "In order to speed up startup time, we recommend tarring this entire directory, and uploading it to S3. At the start of training, this tar file will be downloaded to each SageMaker instance. Downloading as a single archive is much faster than downloading all ~250,000 files.\n",
    "\n",
    "### SageMaker Debugger\n",
    "\n",
    "This training uses SageMaker's \"Bring your own container\" functionality. Using debugger in this scenario requires a small modification to the training script. This imports the Debugger, and wraps the model with it when it finds a json config file in the expected location. The json file is generated when your training job starts based on the configuration you give in this notebook. These changes can be found on lines 31 and 206 of `train.py`.\n",
    "\n",
    "```\n",
    "import smdebug.pytorch as smd\n",
    "from smdebug.core.config_constants import DEFAULT_CONFIG_FILE_PATH\n",
    "\n",
    "...\n",
    "\n",
    "    # wrap model in debugger\n",
    "    if Path(DEFAULT_CONFIG_FILE_PATH).exists() and int(os.environ.get(\"RANK\", 0))==0:\n",
    "        hook = smd.get_hook(create_if_not_exists=True)\n",
    "        hook.register_module(model)\n",
    "```\n",
    "\n",
    "You might also want to wrap your loss function to collect training progress. On many models this can be done in the same file by simply adding `hook.register_loss(loss_func)` after wrapping your model. This case is slightly more complicated, since Yolo uses a more complex loss. In this model, loss is generated in the `compute_loss` function which comes from `utils.general`. In that file, we'll again add the same imports, and wrap the loss fucntions on line 456. \n",
    "\n",
    "```\n",
    "    # Wrap loss functions in debug hook\n",
    "    if Path(DEFAULT_CONFIG_FILE_PATH).exists() and int(os.environ.get(\"RANK\", 0))==0:\n",
    "        hook = smd.get_hook(create_if_not_exists=True)\n",
    "        hook.register_loss(BCEcls)\n",
    "        hook.register_loss(BCEobj)\n",
    "```\n",
    "\n",
    "The `get_hook` class method will check global variables for an existing hook, and grab the same hook we used for the model. Notice we only wrapped the class and object loss functions in this case. This is one limitation for monitoring loss. The debugger expects loss functions to be a subclass of the `torch.module` class. The current model uses GIOU loss for box prediction, which is not written as a subclass of `torch.module`. This is not a major change, and will be updated in a future version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e02839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "from sagemaker import analytics, image_uris\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    DebuggerHookConfig,\n",
    "    TensorBoardOutputConfig,\n",
    "    CollectionConfig,\n",
    "    ProfilerConfig,\n",
    "    FrameworkProfile,\n",
    "    DetailedProfilingConfig,\n",
    "    DataloaderProfilingConfig,\n",
    "    rule_configs,\n",
    ")\n",
    "from smdebug.core.collection import CollectionKeys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052536f2",
   "metadata": {},
   "source": [
    "### S3 Setup\n",
    "\n",
    "The paragraph below sets up your S3 bucket, output locations, and SageMaker job name. This is optional, but makes it easier to keep track of multiple training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "boto_sess = boto3.Session()\n",
    "sm = boto_sess.client('sagemaker')\n",
    "\n",
    "s3_bucket = \"s3://[your S3 bucket]/\"\n",
    "\n",
    "base_job_name = \"[your job name]\"\n",
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "job_name = f\"{base_job_name}-{time_str}\"\n",
    "\n",
    "output_path = os.path.join(s3_bucket, \"sagemaker-output\", date_str, job_name)\n",
    "code_location = os.path.join(s3_bucket, \"sagemaker-code\", date_str, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677e636",
   "metadata": {},
   "source": [
    "### Studio Experiments\n",
    "\n",
    "This paragraph sets up SageMaker experiments to track training in Studio. This is also optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61868820",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Create new experiment\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=base_job_name,\n",
    "        description='Yolo Training',\n",
    "        sagemaker_boto_client=sm)\n",
    "except: # Or reload existing\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=base_job_name,\n",
    "        sagemaker_boto_client=sm)\n",
    "\n",
    "trial = Trial.create(\n",
    "    trial_name=job_name,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm)\n",
    "experiment_config = {\n",
    "    'TrialName': trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "# Configure metric definitions\n",
    "metric_definitions = [\n",
    "    {'Name': 'train_loss_step', 'Regex': 'train_loss_step: [0-9\\\\.]+'},\n",
    "    {'Name': 'train_acc_step', 'Regex': 'train_acc_step: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc51516",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "This will tell the Debugger hook where to store Tensorboard events files. In this case, it will create a `tensorboard` directory in S3 at the output_path specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_output_config = TensorBoardOutputConfig(s3_output_path=os.path.join(output_path, 'tensorboard'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f6bb3",
   "metadata": {},
   "source": [
    "### Debugger Hooks\n",
    "\n",
    "Here we specify the collections for debugger. We collect loss information every 25 steps, and gradients and weights every 500 steps. We don't apply any reductions, so all model gradients and weights will be saved to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_configs=[\n",
    "    CollectionConfig(\n",
    "        name=CollectionKeys.LOSSES,\n",
    "        parameters={\n",
    "            \"save_interval\": \"25\",\n",
    "            # \"reductions\": \"mean\",\n",
    "        }\n",
    "    ),\n",
    "    CollectionConfig(\n",
    "        name=CollectionKeys.GRADIENTS,\n",
    "        parameters={\n",
    "            \"save_interval\": \"500\",\n",
    "            # \"reductions\": \"mean\",\n",
    "        }\n",
    "    ),\n",
    "    CollectionConfig(\n",
    "        name=CollectionKeys.WEIGHTS,\n",
    "        parameters={\n",
    "            \"save_interval\": \"500\",\n",
    "            # \"reductions\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "debugger_hook_config=DebuggerHookConfig(\n",
    "    collection_configs=collection_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5553275",
   "metadata": {},
   "source": [
    "### System Profiler\n",
    "\n",
    "We collect system level performance data every 500 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd30565",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6802cb2",
   "metadata": {},
   "source": [
    "### Model hyperparameters\n",
    "\n",
    "These will be passed to training as command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch-size\": 64,\n",
    "                   \"epochs\": 2,\n",
    "                   \"data\": \"coco_sagemaker.yaml\",\n",
    "                   \"cfg\": \"yolov4-p5.yaml\",\n",
    "                   \"sync-bn\": \"True\",\n",
    "                   \"name\": \"yolov4-p5\",\n",
    "                   \"logdir\": \"/opt/ml/model/\"\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382b180",
   "metadata": {},
   "source": [
    "### Distributed training\n",
    "\n",
    "For distributed training, we'll use ddp and torchrun. SageMaker does not currently have direct support for torchrun, instead favoring mpi. So what we can do is turn off SageMaker's distribution, and set it up ourselves with the `launch_ddp.py` script. Instead of directly calling our `train.py`, this file will setup EFA, grab the SageMaker environment variables for distributed training, and launch our training script as a subprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution=None\n",
    "entry_point=\"launch_ddp.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2296b5",
   "metadata": {},
   "source": [
    "### Cluster config and image\n",
    "\n",
    "Set what type of instance you want, and how many. \n",
    "\n",
    "Grab the account number from the current instance in order to get your image from ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p3.16xlarge'\n",
    "# instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "\n",
    "repo = \"[your ECR repo]\"\n",
    "tag = \"[your image tag]\"\n",
    "account = os.popen(f\"aws sts get-caller-identity --region {region} --endpoint-url https://sts.{region}.amazonaws.com --query Account --output text\").read().strip()\n",
    "image_uri = f\"{account}.dkr.ecr.{region}.amazonaws.com/{repo}:{tag}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86883c",
   "metadata": {},
   "source": [
    "### Setup Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    source_dir=\".\",\n",
    "    entry_point=entry_point,\n",
    "    base_job_name=job_name,\n",
    "    role=get_execution_role(),\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    distribution=distribution,\n",
    "    volume_size=400,\n",
    "    max_run=7200,\n",
    "    hyperparameters=hyperparameters,\n",
    "    image_uri=image_uri,\n",
    "    output_path=os.path.join(output_path, 'training-output'),\n",
    "    checkpoint_s3_uri=os.path.join(output_path, 'training-checkpoints'),\n",
    "    model_dir=os.path.join(output_path, 'training-model'),\n",
    "    code_location=code_location,\n",
    "    ## Debugger parameters\n",
    "    metric_definitions=metric_definitions,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    #rules=rules,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    disable_profiler=False,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    profiler_config=profiler_config,\n",
    "    input_mode='File',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63c929",
   "metadata": {},
   "source": [
    "### Data channels\n",
    "\n",
    "This should be the location of the coco.tar file you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels={\"all_data\": \"s3://[your-s3-bucket]/data/yolo/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c105ab",
   "metadata": {},
   "source": [
    "### Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "estimator.fit(\n",
    "    inputs=channels,\n",
    "    wait=False,\n",
    "    job_name=job_name,\n",
    "    experiment_config=experiment_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefef05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
